# modules/analysis/categorize.py
import json, re
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain.prompts import ChatPromptTemplate
from langchain.schema import HumanMessage
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
from langchain_core.exceptions import OutputParserException
from wordcloud import WordCloud
import streamlit as st # st.progressë¥¼ ìœ„í•´ streamlit ì„í¬íŠ¸ ì¶”ê°€
import time

# 1. í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ + í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ 
keyword_schema = ResponseSchema(name='keywords', description='ì¶”ì¶œëœ í•µì‹¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸')
keyword_parser = StructuredOutputParser.from_response_schemas([keyword_schema])
keyword_prompt = ChatPromptTemplate.from_template("""
                                         
    ë‹¹ì‹ ì€ ì •ì„±ì  ì‘ë‹µ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ì•„ë˜ ì‘ë‹µ ëª©ë¡ì—ì„œ **í•µì‹¬ í‚¤ì›Œë“œ 3~5ê°œ**ë¥¼ ì‹ë³„í•˜ì„¸ìš”.
    {texts}
    JSON ì˜ˆì‹œ: {{ "keywords": ["ì†Œí†µ", "ì±…ì„ê°", "ë¬¸ì œí•´ê²°"] }}
    
""")

category_schema = ResponseSchema(name='categorized', description='í‚¤ì›Œë“œë³„ ì¹´í…Œê³ ë¦¬ ë§¤í•‘')
category_parser = StructuredOutputParser.from_response_schemas([category_schema])
category_prompt = ChatPromptTemplate.from_template("""
                                                 
    ì•„ë˜ í‚¤ì›Œë“œë¥¼ ì£¼ì œë³„ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
    {keywords}
    JSON ì˜ˆì‹œ: [{{"keyword": "ì†Œí†µ", "category": "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜"}}]
    
    ì§€ì¹¨:
    - ì¹´í…Œê³ ë¦¬ëŠ” 'ì»¤ë®¤ë‹ˆì¼€ì´ì…˜', 'ì—…ë¬´íƒœë„', 'ì—­ëŸ‰', 'ì œë„ ë° í™˜ê²½', 'ê¸°íƒ€' 5ê°œë¡œ ì§€ì •í•¨
    - 'ì»¤ë®¤ë‹ˆì¼€ì´ì…˜'ì˜ ì£¼ìš” ì‚¬ë¡€ëŠ” 'ì†Œí†µ, í˜‘ì—…, ë¦¬ë”ì‹­, ì¡°ì§ë¬¸í™” ë“±'ì„
    - 'ì—…ë¬´íƒœë„'ì˜ ì£¼ìš” ì‚¬ë¡€ëŠ” 'ì±…ì„ê°, ì„±ì‹¤, ì—´ì •, ì ê·¹ ë“±'ì„
    - 'ì—­ëŸ‰'ì˜ ì£¼ìš” ì‚¬ë¡€ëŠ” 'í•´ê²°, ì „ë¬¸ì„±, ëŠ¥ë ¥, ì´í•´ë„ ë“±'ì„
    - 'ì œë„ ë° í™˜ê²½'ì˜ ì£¼ìš” ì‚¬ë¡€ëŠ” 'ë³µì§€, ì‹œìŠ¤í…œ, ê·¼ë¬´í™˜ê²½, ì¡°ì§ë¬¸í™”, êµìœ¡ ìš´ì˜, ì›Œë¼ë°¸ ë“±'ì„
    - 'ê¸°íƒ€'ëŠ” ìœ„ ë„¤ ê°€ì§€ì— ëª…í™•íˆ ë¶„ë¥˜ë˜ì§€ ì•ŠëŠ” ì˜ê²¬, ì œì•ˆ, ë‹¨ìˆœ ê°ì • í‘œí˜„, ëª¨í˜¸í•œ ì‘ë‹µ ë“±ì„ í¬í•¨í•¨
    - JSON ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
    - ë¶ˆí•„ìš”í•œ ì„¤ëª… ì—†ì´ JSONë§Œ ì‘ë‹µ

    í‚¤ì›Œë“œ ëª©ë¡:
    {keywords}

    ì¶œë ¥ ì˜ˆì‹œ:
    [
      {{ "keyword": "ì†Œí†µ", "category": "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜}},
      {{ "keyword": "ì±…ì„ê°", "category": "ì—…ë¬´íƒœë„" }}
    ]
    
""")

# ğŸ”¹ 2. í‚¤ì›Œë“œ ì¶”ì¶œ
def process_batch(batch, llm):
    messages = keyword_prompt.format_messages(texts=batch)
    response = llm.invoke(messages)
    raw = response.content

    try:
        return keyword_parser.parse(raw)["keywords"]
    except OutputParserException:
        match = re.search(r"\{.*\}", raw, flags=re.DOTALL)
        if match:
            try:
                parsed = json.loads(match.group(0))
                return parsed.get("keywords", [])
            except:
                return [] 
        return []

def extract_keywords_parallel(texts, llm, chunk_size=5, max_workers=4):
    all_keywords = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(process_batch, texts[i:i+chunk_size], llm)
                   for i in range(0, len(texts), chunk_size)]
        for f in as_completed(futures):
            all_keywords.extend(f.result())
    return all_keywords

# 3. ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
def categorize_keywords_batch(keywords, llm, batch_size=50):
    categorized = []
    for i in range(0, len(keywords), batch_size):
        batch = keywords[i:i+batch_size]
        prompt_text = category_prompt.format_messages(
            keywords=json.dumps(batch, ensure_ascii=False))[0].content
        
        
        response = llm.invoke([HumanMessage(content=prompt_text)])
        raw = response.content

        try:
            parsed = category_parser.parse(raw)
            categorized.extend(parsed)
        except:
            match = re.search(r"\[.*\]", raw, flags=re.DOTALL)
            if match:
                try:
                    categorized.extend(json.loads(match.group(0)))
                except:
                    continue
    return categorized


# 4. í†µí•© í•¨ìˆ˜ 
def run_keyword_analysis(texts, llm):
    # ì§„í–‰ë¥  ë°”ë¥¼ í‘œì‹œí•  ì»¨í…Œì´ë„ˆ
    progress_placeholder = st.empty()
    progress_bar = progress_placeholder.progress(0, text="í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
    
    # í‚¤ì›Œë“œ ì¶”ì¶œ ë¶€ë¶„
    all_keywords = []
    texts_chunks = [texts[i:i + 5] for i in range(0, len(texts), 5)]
    total_chunks = len(texts_chunks)
    
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = {executor.submit(process_batch, chunk, llm): i for i, chunk in enumerate(texts_chunks)}
        
        chunk_count = 0
        for future in as_completed(futures):
            all_keywords.extend(future.result())
            chunk_count += 1
            progress_percent = int(chunk_count / total_chunks * 50) # ì´ 50% ë¹„ì¤‘
            progress_bar.progress(progress_percent, text=f"í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")

    unique_keywords = sorted(set(all_keywords))

    # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ë¶€ë¶„
    total_batches = len(unique_keywords) // 50 + (1 if len(unique_keywords) % 50 != 0 else 0)
    batch_count = 0
    categorized = []
    
    for i in range(0, len(unique_keywords), 50):
        batch = unique_keywords[i:i+50]
        parsed_cat = categorize_keywords_batch(batch, llm)
        categorized.extend(parsed_cat)
        batch_count += 1
        progress_percent = int(50 + (batch_count / total_batches * 50)) # ë‚˜ë¨¸ì§€ 50% ë¹„ì¤‘
        progress_bar.progress(progress_percent, text=f"ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì¤‘...")

    progress_bar.progress(100, text="ë¶„ì„ ì™„ë£Œ!")
    time.sleep(0.5)
    progress_placeholder.empty() # ì§„í–‰ë¥  ë°”ë¥¼ í™”ë©´ì—ì„œ ì œê±°

    df_kw = pd.DataFrame(all_keywords, columns=["keyword"])
    df_kw["category"] = df_kw["keyword"].map({item["keyword"]: item["category"] for item in categorized}).fillna("ê¸°íƒ€")
    freq = df_kw.groupby(["keyword", "category"]).size().reset_index(name="count")


    return freq, {item["keyword"]: item["category"] for item in categorized}

# 5. ì›Œë“œí´ë¼ìš°ë“œ 
def generate_wordcloud_from_freq(freq_df):
    
    # ë¹ˆ ì›Œë“œí´ë¼ìš°ë“œ ë°˜í™˜
    if freq_df.empty or 'keyword' not in freq_df.columns or 'count' not in freq_df.columns: 
        print("âš ï¸ freq_df is empty or missing required columns.")
        return None

    freq_dict = pd.Series(freq_df['count'].values, index=freq_df['keyword']).to_dict()
    wc = WordCloud(width=800, height=400, background_color='white', font_path='/Library/Fonts/AppleGothic.ttf')
    return wc.generate_from_frequencies(freq_dict)